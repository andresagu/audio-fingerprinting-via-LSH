# Audio Fingerprinting using Locality Sensitive Hashing

If you'd like to see my naivety when going into this feel free to take a trip to the `proposed_idea.md`. Now with that context in place, I can say that my aim to try to recreate shazam/spotify/pandora fell pretty flat on it's face but not without some valuable lessons learned along the way and some interesting conclusions drawn. Overall, I'd say this stands as an MVP for creating a LSH table and a case study for audio feature vector extraction/creation. Before I draw any conclusions let me go into the code. First off see the Jupyter Notebook in the `python` directory it's pretty self explanatory and is the first step in getting this up and running. 

### Generating the Feature Vectors
The python code in the jupyter notebook creates our simple feature vectors -  my initial thought here was that given how complex and generally over my head thesee audio features generated by librosa and essentia would really partition these songs, making each of them unique to their feature values...of course there's a but coming. 

5 different genres were used - country, hip-hop, pop, electronic, and rock. I tried to pick royalty free songs that genre wise sounded very different from each other (this was a bit tougher when going into pop and electronic but if you listen to them they're pretty defined!)

Already when looking at the python feature vectors we can see results I did not suspect - the values are not that radically different but the show must go on.

### Implementing an LSH-ish Structure in C++

To understand how this was done here let's take a look at the `SongDict` class - this was used to be the hash table structure that held our feature vectors and the respective songs. 

```
class SongDict{
public:
  SongDict();
  ~SongDict();

  double dot_product(vector<double> feature_vec, vector<double> random_vector);
  vector<vector<int>> randomProjection();
  unsigned int getVecSize();
  vector<vector<double>> parseFeatureVectors();
  void generateRandomVectors();
  void createSongMap();
  vector<vector<string>> parseNameVectors();
  map<vector<int>,vector<string>> GetSongDict();

  
    
private:

//These variables are unchanging thus wouldn't want them to be touched
//The user can get the song dictionary
  vector<vector<double>> random_vec;
  map<vector<int>,vector<string>> song_dict;
};
```

The idea behind LSH is that we want to take "complex" feature vectors and project them onto simpler bit vectors by using random projection. This was actually done in this case by:

1.a - Generating a matrix of random vectors using `void generateRandomVectors()`. This function used C++'s out of the box normal_distribution and random number generator in order to create 15 vectors of the same size as our feature vectors (15 for 15 songs). These random vectors were needed for our hash function which in this case is random projection.
 
1.b - Noting `vector<vector<double>> parseFeatureVectors()`, which is just a quick function that reads our pandas csv export and turns it into a matrix of double values in which the rows represent each song's feature vector and the columns are the different features. 

___

2.a - Random projection...is the dot product of our feature vector * random vector. The idea is we are projecting our feature vector onto a bit representation - a better word for this would be compression. This let's us simplify the features while still preserving the their general nature. In this case the dot product produces another vector with new values which we then flatten to 1 or 0 by checking if the dot product is negative or postive - this is illustrated in the `vector<vector<int>> randomProjection()` function. 

The big key here is that by compressing/projecting our feature vectors we'll have more feature vectors that will share more features since the range of values goes drastically down from unbounded double values to binary values which is key to being able to compare these songs later and pick similarities/differences! 

___

3.a - Finally creating the song dictionary! There were some neat features that C++ maps provided that made this structure the best to use, one of main ones being unique keys! In a hash table the goal is to have unique keys for the objects we store in them, this provides a FINGERPRINT for them and also allows us to efficiently store and partition these objects. Another useful feature maps provided, that was particularly useful for this case, was that map keys can be just about anything and in our case they were vectors. So right out of the box the map stucture could ensure we weren't inserting duplicates or if we were we could handle them gracefully. For this I used chaining to deal with duplicates in the hash table - this meant that if our feature vectors identified songs as feature wise identical (exact same key) then it would add the song to that key's song name vector `map<vector<int>,vector<string>> song_dict` note how song_dict stores the `vector<int>` key and the `vector<string>` value.

#### Unit Tests 

Currently the main app runs through unit tests. They test out the success of functions by processing the sample data and confirming the population of vector matrices. 

The functions themselves are very wordy and will print out what they're doing and share the feature vectors at each step. Running 

`cmake ..`
`make`
from inside the `/build/` directory will create the `run_tests` C++ executable in `/build/`.

exectute `./run_tests` when in the `/build/` directory to see the application in action and get the feature matrices and projected feature matrix.
 
 
 ### Conclusion
 
 Alright, so if it didn't work what's the big idea? I actually suspect it did work but not exactly how I expected and this is the interesting piece to disect. I couldn't seperate the songs into distinct partitions sadly but the structure worked beautifully. I got a really neat projection of the feature vectors into bit representations of themselves BUT they're actually not that different...
 
 ```
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 0 0 1 1 0 1 0 1 0 0 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
1 0 1 1 0 1 0 0 1 0 1 0 1 1 
```

*WHAT HAPPENED??* Well, there are a few hypotheses that I drew from the above experiment and this is why this structure is already such a great MVP for LSH. The explanations break down into lack of data, lack of features, and finally lack of diversity in music itself! Let's begin...

So for starters 15 songs isn't really that much but in order to preserve the sanity of the python kernal and myself I chose to rely on my own judgement of music genre and the categorizations used by https://freemusicarchive.org/. The music extraction function already takes quite a bit to handle the 15 mp3 files that I provided I figured it would be best to keep processing short and choose a variety of genres in order to try to create a decently diverse sample of music to extract features from. Perhaps by injecting my own bias I gravitated to songs that I liked regardless of my attempts to pick diverse songs, most of us usually gravitate to certain styles of music/genres/energy/chromagrams/etc. In a way, this was very affirming since the motivation behind this partially to quantify how objectively diverse our taste actually was and so far my musical diversity is looking really weak. 

So if the pool of songs was much more diverse and unbaised perhaps we'd see a song dictionary with more than just 2 keys and this actually has some validity to it. If you take a moment and listen to the track that is identified as different by the algorithm - `python/music_processing/sample_music/PSRV - Toca Raul - STUDIO RAW.mp3` - you'll notice a really stark contrast and the algorithms can tell.

___

So objectively, my music taste sucks...but what if it doesn't? Perhaps I'm not doing these songs justice by only describing them with 14 features. Of course the 14 features are a gross oversimplification of how spotify algorithms may work to describe songs given that the infrastructure I'm working with is significantly less powerful but I'm also operating without one key aspect, machine learning algorithms. 

The libraries used operate with some machine learning under the hood but there are specific pieces of the librosa and essentia libraries that are directly involved with processing large datasets python sklearn datasets. They provide training datasets that allows for these libraries too more accurately identify features of audio files. This is another way of solving my 15 song dataset issue, these datasets are much larger in comparison and take into account many more features. Perhaps the fingerprint I'm providing just isn't fine enough to create a projection that can distinguish 2 songs from each other - more features, more data, more granualarity.

___

Finally, music today sucks, everythings already been done, no song is special. This is mostly a fun one for those who like to say "real music is dead" whenever a new artist that they don't understand pops up. Satire aside, song writing and music are quite repetitive and formulaic by design 

- the cirlce of fifths is a staple for song writing
- there are only so many keys (24 or 30 depending on who you ask) 12 major scales and 12 minor scales
- pop music exists 

Don't fix what isn't broken. There's only so many ways you can combine sounds to create something that we percieve as pleasing. Of course there are artists that still stretch that but even then there's a limit before it's not even considered music anymore or nobody actually wants to listen to it on their car ride from point a to point b.

This solidifies the interesting and hard conclusion that maybe finding new diverse music is an incredibly hard problem that requires a great amount of computing power and data to even come close to breaking the surface of a perfect recommendation engine. I saw this on a small scale in my tiny sample size of 15 songs but it's totally possible that this problem still arises amongst the giants. We can already classify music in so many quantitative ways, we can pull out numerous feature vectors, inspect waveforms to the most minute granualarity and yet our music tastes end up gravitating in a certain direction or we find ourselves replaying the same songs over and over again in playlists - enjoying things that make us comfortable. 

When looking at it like this, music is far from dead. It seems like true novelty is found in the vibes and personas of emerging artists, and the perfomances and experimentation from past and new artists alike. It makes sense that apps like spotify use social networks to connect us to new sounds and artists as music still remains a very social and alive piece of culture. Perhaps the best we can do is continue to share and expand our spheres of influence as we continue to search for the holy grail of feature vectors to unlock all the music we're missing out on but would love, until then got tell all your friends about your new favorite song this week. 




